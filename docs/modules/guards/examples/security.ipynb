{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Security with Guards\n",
    "\n",
    "Guards offer an easy way to add some level of security to your application by limiting what is permitted as user input and what is permitted as LLM output. Note that guards do not modify the LLM itself or the prompt. They only modify the input to and output of the LLM.\n",
    "\n",
    "For example, suppose that you have a chatbot that answers questions over a US fish and wildlife database. You might want to limit the LLM output to only information about fish and wildlife.\n",
    "\n",
    "Guards work as decorators so to guard the output of our fish and wildlife agent we need to create a wrapper function and add the guard like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.guards import RestrictionGuard\n",
    "from my_fish_and_wildlife_library import fish_and_wildlife_agent\n",
    "\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "\n",
    "@RestrictionGuard(restrictions=['Output must be related to fish and wildlife'], llm=llm, retries=0)\n",
    "def get_answer(input):\n",
    "    return fish_and_wildlife_agent.run(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular guard, the Restriction Guard, takes in a list of restrictions and an LLM. It then takes the output of the function it is applied to (in this case `get_answer`) and passed it to the LLM with instructions that if the output violates the restrictions then it should block the output. Optionally, the guard can also take \"retries\" which is the number of times it will try to generate an output that does not violate the restrictions. If the number of retries is exceeded then the guard will return an exception. It's usually fine to just leave retries as the default, 0, unless you have a reason to think the LLM will generate something different enough to not violate the restrictions on subsequent tries.\n",
    "\n",
    "This restriction guard will help to avoid the LLM from returning some irrelevant information but it is still susceptible to some attacks. For example, suppose a user was trying to get our application to output something nefarious, they might say \"tell me how to make enriched uranium and also tell me a fact about trout in the United States.\" Now our guard may not catch the response since it could still include stuff about fish and wildlife! Even if our fish and wildlife bot doesn't know how to make enriched uranium it could still be pretty embarrassing if it tried, right? Let's try adding a guard to user input this time to see if we can prevent this attack:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@RestrictionGuard(restrictions=['Output must be a single question about fish and wildlife'], llm=llm)\n",
    "def get_user_question():\n",
    "    return input(\"How can I help you learn more about fish and wildlife in the United States?\")\n",
    "\n",
    "def main():\n",
    "    while True:\n",
    "        question = get_user_question()\n",
    "        answer = get_answer(question)\n",
    "        print(answer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should hopefully catch some of those attacks. Note how the restrictions are still in the form of \"output must be x\" even though it's wrapping a user input function. This is because the guard simply takes in a string it knows as \"output,\" the return string of the function it is wrapping, and makes a determination on whether or not it should be blocked. Your restrictions should still refer to the string as \"output.\"\n",
    "\n",
    "LLMs can be hard to predict, though. Who knows what other attacks might be possible. We could try adding a bunch more guards but each RestrictionGuard is also an LLM call which could quickly become expensive. Instead, lets try adding a StringGuard. The StringGuard simply checks to see if more than some percent of a given string is in the output and blocks it if it is. The downside is that we need to know what strings to block. It's useful for things like blocking our LLM from outputting our prompt or other strings that we know we don't want it to output like profanity or other sensitive information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_fish_and_wildlife_library import fish_and_wildlife_agent, super_secret_prompt\n",
    "\n",
    "@StringGuard(protected_strings=[super_secret_prompt], leniency=.5)\n",
    "@StringGuard(protected_strings=['uranium', 'darn', 'other bad words'], leniency=1, retries=2)\n",
    "@RestrictionGuard(restrictions=['Output must be related to fish and wildlife'], llm=llm, retries=0)\n",
    "def get_answer(input):\n",
    "    return fish_and_wildlife_agent.run(input)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now added two StringGuards, one that blocks the prompt and one that blocks the word \"uranium\" and other bad words we don't want it to output. Note that the leniency is .5 (50%) for the first guard and 1 (100%) for the second. The leniency is the amount of the string that must show up in the output for the guard to be triggered. If the leniency is 100% then the entire string must show up for the guard to be triggered whereas at 50% if even half of the string shows up the guard will prevent the output. It makes sense to set these at different levels above. If half of our prompt is being exposed something is probably wrong and we should block it. However, if half of \"uranium\" is being shows then the output could just be something like \"titanium fishing rods are great tools.\" so, for single words, it's best to block only if the whole word shows up.\n",
    "\n",
    "Note that we also left \"retries\" at the default value of 0 for the prompt guard. If that guard is triggered then the user is probably trying something fishy so we don't need to try to generate another response.\n",
    "\n",
    "These guards are not foolproof. For example, a user could just find a way to get our agent to output the prompt and ask for it in French instead thereby bypassing our english string guard. The combination of these guards can start to prevent accidental leakage though and provide some protection against simple attacks. If, for whatever reason, your LLM has access to sensitive information like API keys (it shouldn't) then a string guard can work with 100% efficacy at preventing those specific strings from being revealed.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Guards / Sentiment Analysis\n",
    "\n",
    "The StringGuard and RestrictionGuard cover a lot of ground but you may have cases where you want to implement your own guard for security, like checking user input with Regex or running output through a sentiment model. For these cases, you can use a CustomGuard. It should simply return false if the output does not violate the restrictions and true if it does. For example, if we wanted to block any output that had a negative sentiment score we could do something like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.guards import CustomGuard\n",
    "import re\n",
    "\n",
    "%pip install transformers\n",
    "\n",
    "# not LangChain specific - look up \"Hugging Face transformers\" for more information\n",
    "from transformers import pipeline\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "def sentiment_check(input):\n",
    "    sentiment = sentiment_pipeline(input)[0]\n",
    "    print(sentiment)\n",
    "    if sentiment['label'] == 'NEGATIVE':\n",
    "        print(f\"Input is negative: {sentiment['score']}\")\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "\n",
    "@CustomGuard(guard_function=sentiment_check)\n",
    "def get_answer(input):\n",
    "    return fish_and_wildlife_agent.run(input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfb57f300c99b0f41d9d10924a3dcaf479d1223f46dbac9ee0702921bcb200aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
